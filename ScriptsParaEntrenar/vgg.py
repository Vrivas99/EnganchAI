# -*- coding: utf-8 -*-
"""vgg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h1S36D4swKwpE95xo9XAkCYRAarrUzXO

# Cargar datos
"""

#Cargar drive (ahi estan las imagenes)
from google.colab import drive
drive.mount('/content/drive')

from sklearn.neighbors import KNeighborsClassifier#al final no lo ocupe pero por si se hace knn
import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator

#CSV
import pandas as pd

#Graficar predicciones
import matplotlib.pyplot as plt

#CNN
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# Path to your folder in Google Drive
dataset_folder = '/content/drive/MyDrive/DAISEE2/DataSet'

subset_number = "01"

import os

# Paths to the dataset folders
train_zip_path = os.path.join(dataset_folder, f'TRAIN/SampleTrain_{subset_number}.zip')
val_zip_path = os.path.join(dataset_folder, f'VALIDATION/SampleValidation_{subset_number}.zip')

train_csv = os.path.join(dataset_folder, f'TRAIN/SampleTrain_{subset_number}.csv')
val_csv = os.path.join(dataset_folder, f'VALIDATION/SampleValidation_{subset_number}.csv')

import zipfile
import subprocess

# Run the ls command and capture the output
result = subprocess.run(['ls'], capture_output=True, text=True)

def decompress_zip(zip_path, extract_to):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)

loc = '/content'

# Check if "SampleTrain_" is in the output
if f"SampleTrain_{subset_number}" in result.stdout:
    print(f"El string %'SampleTrain_{subset_number}'% fue detectado, se asume que el dataset fue descomprimido.")
else:
    print(f"El string %'SampleTrain_{subset_number}'% no fue detectado, se asume que el dataset no ha sido descomprimido y por tanto se descomprimira ahora.")
    # Decompress the ZIP files
    decompress_zip(train_zip_path, loc)
    decompress_zip(val_zip_path, loc)

import pandas as pd

def load_csv(csv_file_path):
    return pd.read_csv(csv_file_path)

# Load CSVs (paths in CSVs are already correct)
trainDF = load_csv(train_csv)
valDF = load_csv(val_csv)
trainDF = trainDF.drop('ImagePath', axis=1)
valDF = valDF.drop('ImagePath', axis=1)

#Debido a que las imagenes pueden tener mas de un estado de animo
moodColumns = ['Boredom', 'Engagement', 'Confusion', 'Frustration']#El espacio es un error del csv, dejarlo

# Crea una nueva columna con el mood
def getDominantMood(row):
    # Encuentra la emoción con el valor más alto
    dominantMood = row[moodColumns].idxmax()
    return dominantMood

trainDF['Mood'] = trainDF.apply(getDominantMood, axis=1)
valDF['Mood'] = valDF.apply(getDominantMood, axis=1)

trainDF.head()

#Cargar datos DAiSEE
datagen = ImageDataGenerator(rescale=1.0/255.0,    rotation_range=20,        # Randomly rotate images by 0-20 degrees
    width_shift_range=0.1,    # Randomly shift the image horizontally (10% of the width)
    height_shift_range=0.1,   # Randomly shift the image vertically (10% of the height)
    shear_range=0.1,          # Random shear transformations
    zoom_range=0.1,           # Randomly zoom images in and out
    horizontal_flip=True,     # Randomly flip images horizontally
    fill_mode='nearest'       # Fill in any missing pixels after transformations
)
train_generator = datagen.flow_from_dataframe(
    dataframe=trainDF,
    directory='.',  # Ruta base donde están las imágenes
    x_col='ImageSamplePath',  # Columna con las rutas de las imágenes
    y_col='Mood',  # Columna con las etiquetas
    target_size=(224, 224),  # Tamaño al que redimensionar las imágenes
    batch_size=32,
    class_mode='categorical'  # Dependiendo de cómo están codificadas las etiquetas
)

# No augmentation for validation data, only rescaling
val_datagen = ImageDataGenerator(rescale=1.0/255.0)

validation_generator = val_datagen.flow_from_dataframe(
    dataframe=valDF,
    directory='.',
    x_col='ImageSamplePath',
    y_col='Mood',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

"""# modelo (crear o cargar) [NO EJECUTAR TODO, ELEGIR DENTRO]

##Crear un modelo y guardarlo
"""

from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint

checkpoint_dir = '/content/drive/MyDrive/DAISEE2/vgg/saved_model/'
csv_logger = CSVLogger('/content/drive/MyDrive/DAISEE2/saved_model/vgg/training_log.csv', append=True)
checkpoint_path = os.path.join(checkpoint_dir, 'model_epoch_{epoch:02d}.keras')  # File format for saving
model_checkpoint = ModelCheckpoint(checkpoint_path,
                                    save_weights_only=False,
                                    save_best_only=False,
                                    save_freq='epoch' # Save at the end of each epoch
                                   )

from tensorflow.keras.applications import VGG16

base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(4, activation='sigmoid')
])
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

#Entrenamiento del modelo
history = model.fit(train_generator, epochs=10, validation_data=validation_generator,callbacks=[csv_logger,model_checkpoint])

#Guardado pero con keras (el h5 da una advertencia, entonces por si acaso)
model.save('/content/drive/MyDrive/DAISEE2/saved_model/vgg/modelo_cnn_knn.keras')